{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 48 items from page starting at 1.\n",
      "Scraped 48 items from page starting at 49.\n",
      "Scraped 48 items from page starting at 97.\n",
      "Scraped 48 items from page starting at 145.\n",
      "Scraped 48 items from page starting at 193.\n",
      "Scraped 48 items from page starting at 241.\n",
      "Scraped 48 items from page starting at 289.\n",
      "Scraped 48 items from page starting at 337.\n",
      "Scraped 48 items from page starting at 385.\n",
      "Scraped 48 items from page starting at 433.\n",
      "Scraped 48 items from page starting at 481.\n",
      "Scraped 48 items from page starting at 529.\n",
      "Scraped 48 items from page starting at 577.\n",
      "Scraped 48 items from page starting at 625.\n",
      "Unexpected status: 404. Retrying...\n",
      "Scraped 48 items from page starting at 673.\n",
      "Scraped 48 items from page starting at 721.\n",
      "Scraped 48 items from page starting at 769.\n",
      "Scraped 48 items from page starting at 817.\n",
      "Scraped 48 items from page starting at 865.\n",
      "Scraped 48 items from page starting at 913.\n",
      "Scraped 48 items from page starting at 961.\n",
      "Scraped 48 items from page starting at 1009.\n",
      "Scraped 48 items from page starting at 1057.\n",
      "Scraped 48 items from page starting at 1105.\n",
      "Scraped 48 items from page starting at 1153.\n",
      "Scraped 48 items from page starting at 1201.\n",
      "Scraped 48 items from page starting at 1249.\n",
      "Scraped 48 items from page starting at 1297.\n",
      "Scraped 48 items from page starting at 1345.\n",
      "Scraped 48 items from page starting at 1393.\n",
      "Scraped 48 items from page starting at 1441.\n",
      "Scraped 48 items from page starting at 1489.\n",
      "Scraped 48 items from page starting at 1537.\n",
      "Scraped 48 items from page starting at 1585.\n",
      "Scraped 48 items from page starting at 1633.\n",
      "Scraped 48 items from page starting at 1681.\n",
      "Scraped 48 items from page starting at 1729.\n",
      "Scraped 48 items from page starting at 1777.\n",
      "Scraped 48 items from page starting at 1825.\n",
      "Scraped 48 items from page starting at 1873.\n",
      "Scraped 48 items from page starting at 1921.\n",
      "Scraped 48 items from page starting at 1969.\n",
      "Unexpected status: 404. Retrying...\n",
      "Unexpected status: 404. Retrying...\n",
      "Unexpected status: 404. Retrying...\n",
      "No more items found. Ending scrape.\n",
      "Total Links Collected: 2016\n"
     ]
    }
   ],
   "source": [
    "current_file = Path.cwd()\n",
    "parent_directory = current_file.parent\n",
    "\n",
    "# Base URL without the pagination parameter\n",
    "contract_type = 'venta'\n",
    "investment_type = 'departamento'\n",
    "region = 'metropolitana'\n",
    "base_url = f\"https://www.portalinmobiliario.com/{contract_type}/{investment_type}/{region}/\"\n",
    "start = 1  # Starting point for pagination\n",
    "increment = 48  # Step size for pagination\n",
    "max_pages = 500  # Set a limit to avoid infinite loops\n",
    "\n",
    "# List to store all extracted links\n",
    "all_links = []\n",
    "\n",
    "def fetch_with_rate_limit(url, max_retries=3):\n",
    "    \"\"\"Fetches a URL with retry and rate limiting.\"\"\"\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            elif response.status_code in (429, 503):  # Too Many Requests or Service Unavailable\n",
    "                retry_after = response.headers.get(\"Retry-After\")\n",
    "                retry_after = int(retry_after) if retry_after and retry_after.isdigit() else random.uniform(1, 3)\n",
    "                print(f\"Rate limited. Retrying after {retry_after} seconds...\")\n",
    "                time.sleep(retry_after)\n",
    "            else:\n",
    "                print(f\"Unexpected status: {response.status_code}. Retrying...\")\n",
    "                time.sleep(random.uniform(1, 5))\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "    return None\n",
    "\n",
    "def scrape_page(url):\n",
    "    \"\"\"Scrapes a single page and returns links.\"\"\"\n",
    "    response = fetch_with_rate_limit(url)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        list_items = soup.select(\"main > div > div:nth-of-type(3) > section > ol > li\")\n",
    "        links = [\n",
    "            item.find(\"a\", class_=\"ui-search-result__image ui-search-link\")['href']\n",
    "            for item in list_items if item.find(\"a\", class_=\"ui-search-result__image ui-search-link\")\n",
    "        ]\n",
    "        return links\n",
    "    return []\n",
    "\n",
    "# Main scraping loop\n",
    "for _ in range(max_pages):\n",
    "    current_url = f\"{base_url}_Desde_{start}_OrderId_PRICE_NoIndex_True\"\n",
    "    links = scrape_page(current_url)\n",
    "\n",
    "    if not links:  # Stop if no links are found\n",
    "        print(\"No more items found. Ending scrape.\")\n",
    "        break\n",
    "\n",
    "    all_links.extend(links)\n",
    "    print(f\"Scraped {len(links)} items from page starting at {start}.\")\n",
    "    start += increment\n",
    "    time.sleep(random.uniform(2, 5))  # Random delay to reduce server load\n",
    "\n",
    "cleaned_links = [\n",
    "    re.search(r'(https://www\\.portalinmobiliario\\.com/MLC-\\d+)', url).group(1)\n",
    "    for url in all_links if re.search(r'(https://www\\.portalinmobiliario\\.com/MLC-\\d+)', url)\n",
    "]\n",
    "\n",
    "# Create DataFrame and save results\n",
    "df = pd.DataFrame({\n",
    "    \"contract_type\": contract_type,\n",
    "    \"investment_type\": investment_type,\n",
    "    \"region\": region,\n",
    "    \"url\": cleaned_links\n",
    "})\n",
    "\n",
    "output_file = parent_directory / \"data\" / \"raw\" / 'scraped_links_portal_inmob.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(\"CSV file saved\")\n",
    "\n",
    "print(f\"Total Links Collected: {len(all_links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "current_file = Path.cwd()\n",
    "parent_directory = current_file.parent\n",
    "links_path = parent_directory / \"data\" / \"raw\" / 'scraped_links_portal_inmob.csv'\n",
    "df = pd.read_csv(links_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.portalinmobiliario.com/MLC-2719611378-excelente-depto-en-santiago-sur-_JM\n",
      "Scraping completed and data saved to scraped_apartments.csv.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome()  # Replace with the appropriate WebDriver for your browser\n",
    "\n",
    "# Initialize an empty list to store extracted data\n",
    "scraped_data = []\n",
    "\n",
    "# Iterate over each URL in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url = row['url']\n",
    "    url = r'https://www.portalinmobiliario.com/MLC-2719611378-excelente-depto-en-santiago-sur-_JM'\n",
    "    print(f\"Scraping: {url}\")\n",
    "    try:\n",
    "        # Load the page using Selenium\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page to fully load (adjust timeout and conditions as needed)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"#header > div > div.ui-pdp-header__title-container > h1\"))\n",
    "        )\n",
    "\n",
    "        # Get the rendered page source and parse it with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Extract data using the provided selectors\n",
    "        title = soup.select_one(\"#header > div > div.ui-pdp-header__title-container > h1\")\n",
    "        subtitle = soup.select_one(\"#header > div > div.ui-pdp-header__subtitle > span\")\n",
    "        price = soup.select_one(\"#price > div > div > div > span > span > span.andes-money-amount__fraction\")\n",
    "        common_expenses = soup.select_one(\"#maintenance_fee_vis > p\")\n",
    "        squared_meters = soup.select_one(\"#highlighted_specs_res > div > div:nth-child(1) > span\")\n",
    "        dorms = soup.select_one(\"#highlighted_specs_res > div > div:nth-child(2) > span\")\n",
    "        bathrooms = soup.select_one(\"#highlighted_specs_res > div > div:nth-child(3) > span\")\n",
    "        location = soup.select_one(\"#location > div > div.ui-pdp-media.ui-vip-location__subtitle.ui-pdp-color--BLACK > div > p\")\n",
    "\n",
    "        # Extract coordinates from the map image srcset\n",
    "        map_img = soup.select_one(\"#ui-vip-location__map > div > img\")\n",
    "        coordinates = None\n",
    "        if map_img and 'srcset' in map_img.attrs:\n",
    "            srcset = map_img['srcset']\n",
    "            if \"center=\" in srcset:\n",
    "                coordinates = srcset.split(\"center=\")[1].split(\"&\")[0]  # Extract lat,lng\n",
    "                coordinates = coordinates.replace(\"%2C\", \",\")\n",
    "\n",
    "        # Extract tables\n",
    "        tables = soup.find_all(\"tbody\", class_=\"andes-table__body\")\n",
    "\n",
    "        table_data = {}\n",
    "        for i, table in enumerate(tables, start=1):\n",
    "            rows = table.find_all(\"tr\", class_=\"andes-table__row ui-vpp-striped-specs__row\")\n",
    "            table_entries = []\n",
    "            for row in rows:\n",
    "                header = row.find(\"th\", class_=\"andes-table__header\").text.strip() if row.find(\"th\", class_=\"andes-table__header\") else None\n",
    "                value = row.find(\"td\", class_=\"andes-table__column\").text.strip() if row.find(\"td\", class_=\"andes-table__column\") else None\n",
    "                if header and value:\n",
    "                    table_entries.append({\"property\": header, \"value\": value})\n",
    "            table_data[f\"table_{i}\"] = table_entries  # Store each table with a unique key\n",
    "\n",
    "        # Extract description\n",
    "        description = soup.select_one(\"#description > div > div > div > p\")\n",
    "\n",
    "        # Extract verified seller info\n",
    "        verified_seller = soup.select_one(\"#header > div > div.ui-pdp-seller-validated > p > a\")\n",
    "\n",
    "        # Extract image URL\n",
    "        image = soup.select_one(\"#gallery > div > div > span:nth-child(3) > figure > img\")\n",
    "        image_url = image['src'] if image else None\n",
    "\n",
    "        # Append the extracted data to the list\n",
    "        scraped_data.append({\n",
    "            \"url\": url,\n",
    "            \"title\": title.text.strip() if title else None,\n",
    "            \"subtitle\": subtitle.text.strip() if subtitle else None,\n",
    "            \"price\": int(price.text.strip().replace(\".\", \"\")) if price else None,\n",
    "            \"common_expenses\": common_expenses.text.strip() if common_expenses else None,\n",
    "            \"squared_meters\": squared_meters.text.strip() if squared_meters else None,\n",
    "            \"dorms\": dorms.text.strip() if dorms else None,\n",
    "            \"bathrooms\": bathrooms.text.strip() if bathrooms else None,\n",
    "            \"location\": location.text.strip() if location else None,\n",
    "            \"coordinates\": coordinates,\n",
    "            \"description\": description.text.strip() if description else None,\n",
    "            \"verified_seller\": verified_seller.text.strip() if verified_seller else None,\n",
    "            \"image_url\": image_url,\n",
    "            **table_data\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "    break\n",
    "\n",
    "# Quit the Selenium driver\n",
    "driver.quit()\n",
    "\n",
    "# Convert the scraped data to a DataFrame\n",
    "scraped_df = pd.DataFrame(scraped_data)\n",
    "\n",
    "\n",
    "# Save the scraped data to a CSV file\n",
    "output_file = parent_directory / \"data\" / \"raw\" / 'scraped_apartments_portal_inmob.csv'\n",
    "scraped_df.to_csv(output_file, index=False)\n",
    "print(f\"Scraping completed and data saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 sections with tables.\n"
     ]
    }
   ],
   "source": [
    "# Set up Selenium WebDriver\n",
    "url = r'https://www.portalinmobiliario.com/MLC-1532065067-departamento-nuevo-_JM'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Extract table data (reuse your existing table-scraping logic here)\n",
    "sections = soup.find_all(\"div\", class_=\"ui-vpp-striped-specs__table\")\n",
    "print(f\"Found {len(sections)} sections with tables.\")\n",
    "\n",
    "all_table_data = []\n",
    "for section in sections:\n",
    "    category = section.find(\"h3\", class_=\"ui-vpp-striped-specs__header\")\n",
    "    category_text = category.text.strip() if category else \"Unknown Category\"\n",
    "\n",
    "    table = section.find(\"table\", class_=\"andes-table\")\n",
    "    if not table:\n",
    "        print(f\"No table found in section: {category_text}\")\n",
    "        continue\n",
    "\n",
    "    rows = table.find_all(\"tr\", class_=\"andes-table__row ui-vpp-striped-specs__row\")\n",
    "    for row in rows:\n",
    "        header = row.find(\"th\", class_=\"andes-table__header\")\n",
    "        value = row.find(\"td\", class_=\"andes-table__column\")\n",
    "\n",
    "        header_text = header.text.strip() if header else \"N/A\"\n",
    "        value_text = value.text.strip() if value else \"N/A\"\n",
    "\n",
    "        all_table_data.append({\n",
    "            \"Category\": category_text,\n",
    "            \"Property\": header_text,\n",
    "            \"Value\": value_text\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(all_table_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie banner dismissed.\n",
      "Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010298fac4 cxxbridge1$str$ptr + 3651580\n",
      "1   chromedriver                        0x0000000102988314 cxxbridge1$str$ptr + 3620940\n",
      "2   chromedriver                        0x00000001023f04b4 cxxbridge1$string$len + 89224\n",
      "3   chromedriver                        0x0000000102434898 cxxbridge1$string$len + 368748\n",
      "4   chromedriver                        0x000000010246e0fc cxxbridge1$string$len + 604368\n",
      "5   chromedriver                        0x00000001024290b0 cxxbridge1$string$len + 321668\n",
      "6   chromedriver                        0x0000000102429d00 cxxbridge1$string$len + 324820\n",
      "7   chromedriver                        0x000000010295ae08 cxxbridge1$str$ptr + 3435328\n",
      "8   chromedriver                        0x000000010295e120 cxxbridge1$str$ptr + 3448408\n",
      "9   chromedriver                        0x000000010294217c cxxbridge1$str$ptr + 3333812\n",
      "10  chromedriver                        0x000000010295e9e0 cxxbridge1$str$ptr + 3450648\n",
      "11  chromedriver                        0x0000000102933988 cxxbridge1$str$ptr + 3274432\n",
      "12  chromedriver                        0x00000001029790f4 cxxbridge1$str$ptr + 3558956\n",
      "13  chromedriver                        0x0000000102979270 cxxbridge1$str$ptr + 3559336\n",
      "14  chromedriver                        0x0000000102987f88 cxxbridge1$str$ptr + 3620032\n",
      "15  libsystem_pthread.dylib             0x000000018757df94 _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x0000000187578d34 thread_start + 8\n",
      "\n",
      "Attempt 1: Button not found, refreshing page...\n",
      "No cookie banner found or failed to dismiss it: Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010298fac4 cxxbridge1$str$ptr + 3651580\n",
      "1   chromedriver                        0x0000000102988314 cxxbridge1$str$ptr + 3620940\n",
      "2   chromedriver                        0x00000001023f04b4 cxxbridge1$string$len + 89224\n",
      "3   chromedriver                        0x0000000102434898 cxxbridge1$string$len + 368748\n",
      "4   chromedriver                        0x000000010246e0fc cxxbridge1$string$len + 604368\n",
      "5   chromedriver                        0x00000001024290b0 cxxbridge1$string$len + 321668\n",
      "6   chromedriver                        0x0000000102429d00 cxxbridge1$string$len + 324820\n",
      "7   chromedriver                        0x000000010295ae08 cxxbridge1$str$ptr + 3435328\n",
      "8   chromedriver                        0x000000010295e120 cxxbridge1$str$ptr + 3448408\n",
      "9   chromedriver                        0x000000010294217c cxxbridge1$str$ptr + 3333812\n",
      "10  chromedriver                        0x000000010295e9e0 cxxbridge1$str$ptr + 3450648\n",
      "11  chromedriver                        0x0000000102933988 cxxbridge1$str$ptr + 3274432\n",
      "12  chromedriver                        0x00000001029790f4 cxxbridge1$str$ptr + 3558956\n",
      "13  chromedriver                        0x0000000102979270 cxxbridge1$str$ptr + 3559336\n",
      "14  chromedriver                        0x0000000102987f88 cxxbridge1$str$ptr + 3620032\n",
      "15  libsystem_pthread.dylib             0x000000018757df94 _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x0000000187578d34 thread_start + 8\n",
      "\n",
      "Button clicked successfully. Proceeding...\n",
      "Found 5 sections with tables.\n"
     ]
    }
   ],
   "source": [
    "# Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome()  # Ensure the appropriate WebDriver is installed\n",
    "\n",
    "# Target URL\n",
    "url = r'https://www.portalinmobiliario.com/MLC-1532065067'\n",
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "    # Define the maximum number of attempts\n",
    "    max_attempts = 5\n",
    "    attempts = 0\n",
    "    element_found = False\n",
    "\n",
    "    while attempts < max_attempts and not element_found:\n",
    "        try:\n",
    "            # Wait for the cookie disclaimer to be present\n",
    "            cookie_banner = WebDriverWait(driver, 3).until(\n",
    "                EC.presence_of_element_located((By.ID, \"newCookieDisclaimerBanner\"))\n",
    "            )\n",
    "            # Attempt to close the banner\n",
    "            close_button = cookie_banner.find_element(By.TAG_NAME, \"button\")  # Update selector as needed\n",
    "            close_button.click()\n",
    "            print(\"Cookie banner dismissed.\")\n",
    "        except Exception as e:\n",
    "            print(\"No cookie banner found or failed to dismiss it:\", e)\n",
    "\n",
    "        try:\n",
    "            # Wait for the button to be present\n",
    "            button = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"button.ui-pdp-collapsable__action\"))\n",
    "            )\n",
    "            # Click the button to expand the content\n",
    "            button.click()\n",
    "            element_found = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            print(f\"Attempt {attempts + 1}: Button not found, refreshing page...\")\n",
    "            attempts += 1\n",
    "            driver.refresh()  # Refresh the page\n",
    "            time.sleep(2)  # Wait for the page to reload\n",
    "\n",
    "    if not element_found:\n",
    "        print(\"Button not found after maximum attempts.\")\n",
    "    else:\n",
    "        print(\"Button clicked successfully. Proceeding...\")\n",
    "        time.sleep(5)\n",
    "\n",
    "    # Further processing, such as parsing the page or waiting for the next element\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    sections = soup.find_all(\"div\", class_=\"ui-vpp-striped-specs__table\")\n",
    "    print(f\"Found {len(sections)} sections with tables.\")\n",
    "\n",
    "    all_table_data = []\n",
    "    for section in sections:\n",
    "        category = section.find(\"h3\", class_=\"ui-vpp-striped-specs__header\")\n",
    "        category_text = category.text.strip() if category else \"Unknown Category\"\n",
    "\n",
    "        table = section.find(\"table\", class_=\"andes-table\")\n",
    "        if not table:\n",
    "            print(f\"No table found in section: {category_text}\")\n",
    "            continue\n",
    "\n",
    "        rows = table.find_all(\"tr\", class_=\"andes-table__row ui-vpp-striped-specs__row\")\n",
    "        for row in rows:\n",
    "            header = row.find(\"th\", class_=\"andes-table__header\")\n",
    "            value = row.find(\"td\", class_=\"andes-table__column\")\n",
    "\n",
    "            header_text = header.text.strip() if header else \"N/A\"\n",
    "            value_text = value.text.strip() if value else \"N/A\"\n",
    "\n",
    "            all_table_data.append({\n",
    "                \"Category\": category_text,\n",
    "                \"Property\": header_text,\n",
    "                \"Value\": value_text\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(all_table_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Quit the WebDriver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Property</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Superficie total</td>\n",
       "      <td>4 m²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Superficie útil</td>\n",
       "      <td>60 m²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Superficie de terraza</td>\n",
       "      <td>0 m²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Ambientes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Dormitorios</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Baños</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Estacionamientos</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Bodegas</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Cantidad de pisos</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Departamentos por piso</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Número de piso de la unidad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Tipo de departamento</td>\n",
       "      <td>Departamento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Orientación</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Antigüedad</td>\n",
       "      <td>2 años</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Principales</td>\n",
       "      <td>Gastos comunes</td>\n",
       "      <td>120.000 CLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Comodidades y equipamiento</td>\n",
       "      <td>Acceso a internet</td>\n",
       "      <td>Sí</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Comodidades y equipamiento</td>\n",
       "      <td>Gimnasio</td>\n",
       "      <td>Sí</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Comodidades y equipamiento</td>\n",
       "      <td>Ascensor</td>\n",
       "      <td>Sí</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ambientes</td>\n",
       "      <td>Baño de visitas</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ambientes</td>\n",
       "      <td>Terraza</td>\n",
       "      <td>Sí</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ambientes</td>\n",
       "      <td>Balcón</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Ambientes</td>\n",
       "      <td>Jardín</td>\n",
       "      <td>Sí</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Ambientes</td>\n",
       "      <td>Parrilla</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ambientes</td>\n",
       "      <td>Piscina</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Servicios</td>\n",
       "      <td>Calefacción</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Condiciones especiales</td>\n",
       "      <td>Uso comercial</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Category                     Property         Value\n",
       "0                  Principales             Superficie total          4 m²\n",
       "1                  Principales              Superficie útil         60 m²\n",
       "2                  Principales        Superficie de terraza          0 m²\n",
       "3                  Principales                    Ambientes             0\n",
       "4                  Principales                  Dormitorios             2\n",
       "5                  Principales                        Baños             2\n",
       "6                  Principales             Estacionamientos             0\n",
       "7                  Principales                      Bodegas             0\n",
       "8                  Principales            Cantidad de pisos            20\n",
       "9                  Principales       Departamentos por piso          1101\n",
       "10                 Principales  Número de piso de la unidad             0\n",
       "11                 Principales         Tipo de departamento  Departamento\n",
       "12                 Principales                  Orientación             N\n",
       "13                 Principales                   Antigüedad        2 años\n",
       "14                 Principales               Gastos comunes   120.000 CLP\n",
       "15  Comodidades y equipamiento            Acceso a internet            Sí\n",
       "16  Comodidades y equipamiento                     Gimnasio            Sí\n",
       "17  Comodidades y equipamiento                     Ascensor            Sí\n",
       "18                   Ambientes              Baño de visitas            No\n",
       "19                   Ambientes                      Terraza            Sí\n",
       "20                   Ambientes                       Balcón            No\n",
       "21                   Ambientes                       Jardín            Sí\n",
       "22                   Ambientes                     Parrilla            No\n",
       "23                   Ambientes                      Piscina            No\n",
       "24                   Servicios                  Calefacción            No\n",
       "25      Condiciones especiales                Uso comercial            No"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
