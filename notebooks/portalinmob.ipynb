{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 48 items from page starting at 1.\n",
      "Scraped 48 items from page starting at 49.\n",
      "Scraped 48 items from page starting at 97.\n",
      "Scraped 48 items from page starting at 145.\n",
      "Scraped 48 items from page starting at 193.\n",
      "Unexpected status: 404. Retrying...\n",
      "Scraped 48 items from page starting at 241.\n",
      "Scraped 48 items from page starting at 289.\n",
      "Scraped 48 items from page starting at 337.\n",
      "Scraped 48 items from page starting at 385.\n",
      "Scraped 48 items from page starting at 433.\n",
      "Scraped 48 items from page starting at 481.\n",
      "Unexpected status: 404. Retrying...\n",
      "Scraped 48 items from page starting at 529.\n",
      "Scraped 48 items from page starting at 577.\n",
      "Scraped 48 items from page starting at 625.\n",
      "Scraped 48 items from page starting at 673.\n",
      "Unexpected status: 404. Retrying...\n",
      "Scraped 48 items from page starting at 721.\n",
      "Scraped 48 items from page starting at 769.\n",
      "Scraped 48 items from page starting at 817.\n",
      "Scraped 48 items from page starting at 865.\n",
      "Scraped 48 items from page starting at 913.\n",
      "Scraped 48 items from page starting at 961.\n",
      "Scraped 48 items from page starting at 1009.\n",
      "Unexpected status: 404. Retrying...\n",
      "Scraped 48 items from page starting at 1057.\n",
      "Scraped 48 items from page starting at 1105.\n",
      "Scraped 48 items from page starting at 1153.\n",
      "Scraped 48 items from page starting at 1201.\n",
      "Scraped 48 items from page starting at 1249.\n",
      "Scraped 48 items from page starting at 1297.\n",
      "Unexpected status: 404. Retrying...\n",
      "Scraped 48 items from page starting at 1345.\n",
      "Scraped 48 items from page starting at 1393.\n",
      "Scraped 48 items from page starting at 1441.\n",
      "Scraped 48 items from page starting at 1489.\n",
      "Scraped 48 items from page starting at 1537.\n",
      "Scraped 48 items from page starting at 1585.\n",
      "Scraped 48 items from page starting at 1633.\n",
      "Scraped 48 items from page starting at 1681.\n",
      "Scraped 48 items from page starting at 1729.\n",
      "Scraped 48 items from page starting at 1777.\n",
      "Scraped 48 items from page starting at 1825.\n",
      "Scraped 48 items from page starting at 1873.\n",
      "Scraped 48 items from page starting at 1921.\n",
      "Scraped 48 items from page starting at 1969.\n",
      "Unexpected status: 404. Retrying...\n",
      "Unexpected status: 404. Retrying...\n",
      "Unexpected status: 404. Retrying...\n",
      "No more items found. Ending scrape.\n",
      "Total Links Collected: 2016\n"
     ]
    }
   ],
   "source": [
    "current_file = Path.cwd()\n",
    "parent_directory = current_file.parent\n",
    "\n",
    "# Base URL without the pagination parameter\n",
    "contract_type = 'venta'\n",
    "investment_type = 'departamento'\n",
    "region = 'metropolitana'\n",
    "base_url = f\"https://www.portalinmobiliario.com/{contract_type}/{investment_type}/{region}/\"\n",
    "start = 1  # Starting point for pagination\n",
    "increment = 48  # Step size for pagination\n",
    "max_pages = 500  # Set a limit to avoid infinite loops\n",
    "\n",
    "# List to store all extracted links\n",
    "all_links = []\n",
    "\n",
    "def fetch_with_rate_limit(url, max_retries=3):\n",
    "    \"\"\"Fetches a URL with retry and rate limiting.\"\"\"\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            elif response.status_code in (429, 503):  # Too Many Requests or Service Unavailable\n",
    "                retry_after = response.headers.get(\"Retry-After\")\n",
    "                retry_after = int(retry_after) if retry_after and retry_after.isdigit() else random.uniform(1, 3)\n",
    "                print(f\"Rate limited. Retrying after {retry_after} seconds...\")\n",
    "                time.sleep(retry_after)\n",
    "            else:\n",
    "                print(f\"Unexpected status: {response.status_code}. Retrying...\")\n",
    "                time.sleep(random.uniform(1, 5))\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "    return None\n",
    "\n",
    "def scrape_page(url):\n",
    "    \"\"\"Scrapes a single page and returns links.\"\"\"\n",
    "    response = fetch_with_rate_limit(url)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        list_items = soup.select(\"main > div > div:nth-of-type(3) > section > ol > li\")\n",
    "        links = [\n",
    "            item.find(\"a\", class_=\"ui-search-result__image ui-search-link\")['href']\n",
    "            for item in list_items if item.find(\"a\", class_=\"ui-search-result__image ui-search-link\")\n",
    "        ]\n",
    "        return links\n",
    "    return []\n",
    "\n",
    "# Main scraping loop\n",
    "for _ in range(max_pages):\n",
    "    current_url = f\"{base_url}_Desde_{start}_OrderId_PRICE_NoIndex_True\"\n",
    "    links = scrape_page(current_url)\n",
    "\n",
    "    if not links:  # Stop if no links are found\n",
    "        print(\"No more items found. Ending scrape.\")\n",
    "        break\n",
    "\n",
    "    all_links.extend(links)\n",
    "    print(f\"Scraped {len(links)} items from page starting at {start}.\")\n",
    "    start += increment\n",
    "    time.sleep(random.uniform(2, 5))  # Random delay to reduce server load\n",
    "\n",
    "cleaned_links = [\n",
    "    re.search(r'(https://www\\.portalinmobiliario\\.com/MLC-\\d+)', url).group(1)\n",
    "    for url in all_links if re.search(r'(https://www\\.portalinmobiliario\\.com/MLC-\\d+)', url)\n",
    "]\n",
    "\n",
    "# Create DataFrame and save results\n",
    "df = pd.DataFrame({\n",
    "    \"contract_type\": contract_type,\n",
    "    \"investment_type\": investment_type,\n",
    "    \"region\": region,\n",
    "    \"url\": cleaned_links\n",
    "})\n",
    "\n",
    "\n",
    "output_file = parent_directory / \"data\" / \"raw\" / 'scraped_links_portal_inmob.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Total Links Collected: {len(all_links)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "current_file = Path.cwd()\n",
    "parent_directory = current_file.parent\n",
    "links_path = parent_directory / \"data\" / \"raw\" / 'scraped_links_portal_inmob.csv'\n",
    "df = pd.read_csv(links_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.portalinmobiliario.com/MLC-1544145281\n",
      "Scraping completed and data saved to 'scraped_apartments.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store extracted data\n",
    "scraped_data = []\n",
    "\n",
    "# Iterate over each URL in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url = row['url']\n",
    "    url = r'https://www.portalinmobiliario.com/MLC-1544145281'\n",
    "    print(f\"Scraping: {url}\")\n",
    "    try:\n",
    "        # Fetch the page content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract data using the provided selectors\n",
    "        title = soup.select_one(\"#header > div > div.ui-pdp-header__title-container > h1\")\n",
    "        subtitle = soup.select_one(\"#header > div > div.ui-pdp-header__subtitle > span\")\n",
    "        price = soup.select_one(\"#price > div > div > div > span > span > span.andes-money-amount__fraction\")\n",
    "        common_expenses = soup.select_one(\"#maintenance_fee_vis > p\")\n",
    "        squared_meters = soup.select_one(\"#highlighted_specs_res > div > div:nth-child(1) > span\")\n",
    "        dorms = soup.select_one(\"#highlighted_specs_res > div > div:nth-child(2) > span\")\n",
    "        bathrooms = soup.select_one(\"#highlighted_specs_res > div > div:nth-child(3) > span\")\n",
    "        location = soup.select_one(\"#location > div > div.ui-pdp-media.ui-vip-location__subtitle.ui-pdp-color--BLACK > div > p\")\n",
    "\n",
    "        # Extract coordinates from the map image srcset\n",
    "        map_img = soup.select_one(\"#ui-vip-location__map > div > img\")\n",
    "        coordinates = None\n",
    "        if map_img and 'srcset' in map_img.attrs:\n",
    "            srcset = map_img['srcset']\n",
    "            if \"center=\" in srcset:\n",
    "                coordinates = srcset.split(\"center=\")[1].split(\"&\")[0]  # Extract lat,lng\n",
    "                coordinates = coordinates.replace(\"%2C\", \",\")\n",
    "\n",
    "        # Extract tables\n",
    "        table1 = soup.select_one(\"#highlighted_specs_attrs > div > div > div > div:nth-child(1) > div:nth-child(1) > table\")\n",
    "        table2 = soup.select_one(\"#highlighted_specs_attrs > div > div > div > div:nth-child(2) > div:nth-child(1) > table\")\n",
    "        table3 = soup.select_one(\"#highlighted_specs_attrs > div > div > div > div:nth-child(1) > div:nth-child(2) > table\")\n",
    "        table4 = soup.select_one(\"#highlighted_specs_attrs > div > div > div > div:nth-child(2) > div:nth-child(2) > table\")\n",
    "        table5 = soup.select_one(\"#highlighted_specs_attrs > div > div > div > div:nth-child(2) > div:nth-child(3) > table\")\n",
    "        table6 = soup.select_one(\"#highlighted_specs_attrs > div > div > div > div:nth-child(2) > div:nth-child(4) > table\")\n",
    "\n",
    "        # Extract description\n",
    "        description = soup.select_one(\"#description > div > div > div > p\")\n",
    "\n",
    "        # Extract verified seller info\n",
    "        verified_seller = soup.select_one(\"#header > div > div.ui-pdp-seller-validated > p > a\")\n",
    "\n",
    "        # Extract image URL\n",
    "        image = soup.select_one(\"#gallery > div > div > span:nth-child(3) > figure > img\")\n",
    "        image_url = image['src'] if image else None\n",
    "\n",
    "        # Append the extracted data to the list\n",
    "        scraped_data.append({\n",
    "            \"url\": url,\n",
    "            \"title\": title.text.strip() if title else None,\n",
    "            \"subtitle\": subtitle.text.strip() if subtitle else None,\n",
    "            \"price\": int(price.text.strip().replace(\".\",\"\")) if price else None,\n",
    "            \"common_expenses\": common_expenses.text.strip() if common_expenses else None,\n",
    "            \"squared_meters\": squared_meters.text.strip() if squared_meters else None,\n",
    "            \"dorms\": dorms.text.strip() if dorms else None,\n",
    "            \"bathrooms\": bathrooms.text.strip() if bathrooms else None,\n",
    "            \"location\": location.text.strip() if location else None,\n",
    "            \"coordinates\": coordinates,\n",
    "            \"table1\": str(table1) if table1 else None,\n",
    "            \"table2\": str(table2) if table2 else None,\n",
    "            \"table3\": str(table3) if table3 else None,\n",
    "            \"table4\": str(table4) if table4 else None,\n",
    "            \"table5\": str(table5) if table5 else None,\n",
    "            \"table6\": str(table6) if table6 else None,\n",
    "            \"description\": description.text.strip() if description else None,\n",
    "            \"verified_seller\": verified_seller.text.strip() if verified_seller else None,\n",
    "            \"image_url\": image_url\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "# Convert the scraped data to a DataFrame\n",
    "scraped_df = pd.DataFrame(scraped_data)\n",
    "\n",
    "output_file = parent_directory / \"data\" / \"raw\" / 'scraped_apartments_portal_inmob.csv'\n",
    "\n",
    "# Save the scraped data to a CSV\n",
    "scraped_df.to_csv(output_file, index=False)\n",
    "print(\"Scraping completed and data saved to 'scraped_apartments.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
