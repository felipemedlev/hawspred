{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 48 items from page starting at 1.\n",
      "Scraped 48 items from page starting at 49.\n",
      "Scraped 48 items from page starting at 97.\n",
      "Scraped 48 items from page starting at 145.\n",
      "Scraped 48 items from page starting at 193.\n",
      "Scraped 48 items from page starting at 241.\n",
      "Scraped 48 items from page starting at 289.\n",
      "Scraped 48 items from page starting at 337.\n",
      "Scraped 48 items from page starting at 385.\n",
      "Scraped 48 items from page starting at 433.\n",
      "Scraped 48 items from page starting at 481.\n",
      "Scraped 48 items from page starting at 529.\n",
      "Scraped 48 items from page starting at 577.\n",
      "Scraped 48 items from page starting at 625.\n",
      "Scraped 48 items from page starting at 673.\n",
      "Scraped 48 items from page starting at 721.\n",
      "Scraped 48 items from page starting at 769.\n",
      "Scraped 48 items from page starting at 817.\n",
      "Scraped 48 items from page starting at 865.\n",
      "Scraped 48 items from page starting at 913.\n",
      "Scraped 48 items from page starting at 961.\n",
      "Scraped 48 items from page starting at 1009.\n",
      "Scraped 48 items from page starting at 1057.\n",
      "Scraped 48 items from page starting at 1105.\n",
      "Scraped 48 items from page starting at 1153.\n",
      "Scraped 48 items from page starting at 1201.\n",
      "Scraped 48 items from page starting at 1249.\n",
      "Scraped 48 items from page starting at 1297.\n",
      "Scraped 48 items from page starting at 1345.\n",
      "Scraped 48 items from page starting at 1393.\n",
      "Scraped 48 items from page starting at 1441.\n",
      "Scraped 48 items from page starting at 1489.\n",
      "Scraped 48 items from page starting at 1537.\n",
      "Scraped 48 items from page starting at 1585.\n",
      "Scraped 48 items from page starting at 1633.\n",
      "Scraped 48 items from page starting at 1681.\n",
      "Scraped 48 items from page starting at 1729.\n",
      "Scraped 48 items from page starting at 1777.\n",
      "Scraped 48 items from page starting at 1825.\n",
      "Scraped 48 items from page starting at 1873.\n",
      "Scraped 48 items from page starting at 1921.\n",
      "Scraped 48 items from page starting at 1969.\n",
      "Unexpected status: 404. Retrying...\n",
      "Unexpected status: 404. Retrying...\n",
      "Unexpected status: 404. Retrying...\n",
      "Unexpected status: 404. Retrying...\n",
      "Unexpected status: 404. Retrying...\n",
      "No more items found. Ending scrape.\n",
      "Total Links Collected: 2016\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "current_file = Path(__file__)\n",
    "parent_directory = current_file.parent\n",
    "\n",
    "# Base URL without the pagination parameter\n",
    "contract_type = 'venta'\n",
    "investment_type = 'departamento'\n",
    "region = 'metropolitana'\n",
    "base_url = f\"https://www.portalinmobiliario.com/{contract_type}/{investment_type}/{region}/\"\n",
    "start = 1  # Starting point for pagination\n",
    "increment = 48  # Step size for pagination\n",
    "max_pages = 500  # Set a limit to avoid infinite loops\n",
    "\n",
    "# List to store all extracted links\n",
    "all_links = []\n",
    "\n",
    "def fetch_with_rate_limit(url, max_retries=3):\n",
    "    \"\"\"Fetches a URL with retry and rate limiting.\"\"\"\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            elif response.status_code in (429, 503):  # Too Many Requests or Service Unavailable\n",
    "                retry_after = response.headers.get(\"Retry-After\")\n",
    "                retry_after = int(retry_after) if retry_after and retry_after.isdigit() else random.uniform(1, 3)\n",
    "                print(f\"Rate limited. Retrying after {retry_after} seconds...\")\n",
    "                time.sleep(retry_after)\n",
    "            else:\n",
    "                print(f\"Unexpected status: {response.status_code}. Retrying...\")\n",
    "                time.sleep(random.uniform(1, 5))\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "    return None\n",
    "\n",
    "def scrape_page(url):\n",
    "    \"\"\"Scrapes a single page and returns links.\"\"\"\n",
    "    response = fetch_with_rate_limit(url)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        list_items = soup.select(\"main > div > div:nth-of-type(3) > section > ol > li\")\n",
    "        links = [\n",
    "            item.find(\"a\", class_=\"ui-search-result__image ui-search-link\")['href']\n",
    "            for item in list_items if item.find(\"a\", class_=\"ui-search-result__image ui-search-link\")\n",
    "        ]\n",
    "        return links\n",
    "    return []\n",
    "\n",
    "# Main scraping loop\n",
    "for _ in range(max_pages):\n",
    "    current_url = f\"{base_url}_Desde_{start}_OrderId_PRICE_NoIndex_True\"\n",
    "    links = scrape_page(current_url)\n",
    "\n",
    "    if not links:  # Stop if no links are found\n",
    "        print(\"No more items found. Ending scrape.\")\n",
    "        break\n",
    "\n",
    "    all_links.extend(links)\n",
    "    print(f\"Scraped {len(links)} items from page starting at {start}.\")\n",
    "    start += increment\n",
    "    time.sleep(random.uniform(2, 5))  # Random delay to reduce server load\n",
    "\n",
    "cleaned_links = [\n",
    "    re.search(r'(https://www\\.portalinmobiliario\\.com/MLC-\\d+)', url).group(1)\n",
    "    for url in all_links if re.search(r'(https://www\\.portalinmobiliario\\.com/MLC-\\d+)', url)\n",
    "]\n",
    "\n",
    "# Create DataFrame and save results\n",
    "df = pd.DataFrame({\n",
    "    \"contract_type\": contract_type,\n",
    "    \"investment_type\": investment_type,\n",
    "    \"region\": region,\n",
    "    \"url\": all_links\n",
    "})\n",
    "\n",
    "\n",
    "output_file = \"scraped_links.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Total Links Collected: {len(all_links)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "current_file = Path.cwd()\n",
    "parent_directory = current_file.parent\n",
    "\n",
    "output_file = parent_directory / \"data\" / \"raw\" / 'scraped_links_portal_inmob.csv'\n",
    "df.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
