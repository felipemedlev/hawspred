{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "current_file = Path.cwd()\n",
    "parent_directory = current_file.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL without the pagination parameter\n",
    "transaction_type = 'venta'\n",
    "property_type = 'departamento'\n",
    "listing_category = 'propiedades-usadas'\n",
    "region_name = 'metropolitana'\n",
    "base_url = f\"https://www.portalinmobiliario.com/{transaction_type}/{property_type}/{listing_category}/{region_name}/\"\n",
    "\n",
    "start = 1  # Starting point for pagination\n",
    "increment = 48  # Step size for pagination\n",
    "max_pages = 500  # Set a limit to avoid infinite loops\n",
    "\n",
    "# List to store all extracted links\n",
    "all_links = []\n",
    "\n",
    "def fetch_with_rate_limit(url, max_retries=3):\n",
    "    \"\"\"Fetches a URL with retry and rate limiting.\"\"\"\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            elif response.status_code in (429, 503):  # Too Many Requests or Service Unavailable\n",
    "                retry_after = response.headers.get(\"Retry-After\")\n",
    "                retry_after = int(retry_after) if retry_after and retry_after.isdigit() else random.uniform(1, 3)\n",
    "                print(f\"Rate limited. Retrying after {retry_after} seconds...\")\n",
    "                time.sleep(retry_after)\n",
    "            else:\n",
    "                print(f\"Unexpected status: {response.status_code}. Retrying...\")\n",
    "                time.sleep(random.uniform(1, 5))\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "    return None\n",
    "\n",
    "def scrape_page(url):\n",
    "    \"\"\"Scrapes a single page and returns links.\"\"\"\n",
    "    response = fetch_with_rate_limit(url)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        list_items = soup.select(\"main > div > div:nth-of-type(3) > section > ol > li\")\n",
    "        links = [\n",
    "            item.find(\"a\", class_=\"ui-search-result__image ui-search-link\")['href']\n",
    "            for item in list_items if item.find(\"a\", class_=\"ui-search-result__image ui-search-link\")\n",
    "        ]\n",
    "        return links\n",
    "    return []\n",
    "\n",
    "# Main scraping loop\n",
    "for _ in range(max_pages):\n",
    "    current_url = f\"{base_url}_Desde_{start}_OrderId_PRICE_NoIndex_True\"\n",
    "    links = scrape_page(current_url)\n",
    "\n",
    "    if not links:  # Stop if no links are found\n",
    "        print(\"No more items found. Ending scrape.\")\n",
    "        break\n",
    "\n",
    "    all_links.extend(links)\n",
    "    print(f\"Scraped {len(links)} items from page starting at {start}.\")\n",
    "    start += increment\n",
    "    time.sleep(random.uniform(2, 5))  # Random delay to reduce server load\n",
    "\n",
    "cleaned_links = [\n",
    "    re.search(r'(https://www\\.portalinmobiliario\\.com/MLC-\\d+)', url).group(1)\n",
    "    for url in all_links if re.search(r'(https://www\\.portalinmobiliario\\.com/MLC-\\d+)', url)\n",
    "]\n",
    "\n",
    "# Create DataFrame and save results\n",
    "links_df = pd.DataFrame({\n",
    "    \"transaction_type\": transaction_type,\n",
    "    \"property_type\": property_type,\n",
    "    \"listing_category\": listing_category,\n",
    "    \"region_name\": region_name,\n",
    "    \"url\": cleaned_links\n",
    "})\n",
    "\n",
    "output_file = parent_directory / \"data\" / \"raw\" / 'scraped_links_portal_inmob.csv'\n",
    "links_df.to_csv(output_file, index=False)\n",
    "print(\"CSV file saved\")\n",
    "\n",
    "print(f\"Total Links Collected: {len(all_links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_file = Path.cwd()\n",
    "parent_directory = current_file.parent\n",
    "links_path = parent_directory / \"data\" / \"raw\" / 'scraped_links_portal_inmob.csv'\n",
    "links_df = pd.read_csv(links_path)\n",
    "try:\n",
    "    scraped_path = parent_directory / \"data\" / \"raw\" / 'scraped_apartments_portal_inmob.csv'\n",
    "    scraped_df = pd.read_csv(scraped_path)\n",
    "    # Narrow down links to scrape for apartments which haven't been scraped before\n",
    "    remaining_links_df = links_df[~links_df['url'].isin(scraped_df['url'])].reset_index(drop=True)\n",
    "except Exception as e:\n",
    "    print(\"no scraped_df csv\")\n",
    "    print(f\"error: {str(e)}\")\n",
    "\n",
    "# # FIX: Add apartments with no currency unit\n",
    "# no_curr_scraped_links_df = scraped_df[scraped_df['currency'].isna()][['url']].copy()\n",
    "# scraped_df = scraped_df[~scraped_df['currency'].isna()]\n",
    "# no_curr_scraped_links_df['transaction_type'] = transaction_type\n",
    "# no_curr_scraped_links_df['property_type'] = property_type\n",
    "# no_curr_scraped_links_df['listing_category'] = listing_category\n",
    "# no_curr_scraped_links_df['region_name'] = region_name\n",
    "# cols_in_order = ['transaction_type', 'property_type', 'listing_category', 'region_name', 'url']\n",
    "# no_curr_scraped_links_df = no_curr_scraped_links_df[cols_in_order]\n",
    "# remaining_links_df = pd.concat([remaining_links_df, no_curr_scraped_links_df], ignore_index=True)\n",
    "# remaining_links_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.portalinmobiliario.com/MLC-2726723992\n",
      "Entro\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-2772756714\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-2777065320\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-2778446752\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-2778554378\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-1544243983\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-1537685835\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-2679102934\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-2786159288\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-1553723949\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-2770777530\n",
      "Entro\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-2789092370\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-1550859743\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-1551538641\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-1511600233\n",
      "Scraping: https://www.portalinmobiliario.com/MLC-2794613248\n"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "\n",
    "# Initialize an empty list to store extracted data\n",
    "scraped_data = []\n",
    "\n",
    "# Iterate over each URL in the DataFrame\n",
    "batch_size = 10\n",
    "driver = None\n",
    "for index, row in remaining_links_df.iterrows():\n",
    "    url = row['url']\n",
    "    print(f\"Scraping: {url}\")\n",
    "    # Reinitialize the driver every 'batch_size' iterations\n",
    "    if index % batch_size == 0:\n",
    "        print(\"Entro\")\n",
    "        if driver is not None:\n",
    "            driver.quit()  # Quit the old driver if it exists\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "    try:\n",
    "        # Load the page using Selenium to bypass anti-bot measures\n",
    "        driver.get(url)\n",
    "        # Must refresh to bypass anti-bot measures\n",
    "        driver.refresh()\n",
    "        time.sleep(random.uniform(0, 1))\n",
    "        WebDriverWait(driver, 3).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, \"ui-vpp-striped-specs__table\"))\n",
    "        )\n",
    "        # Get the rendered page source and parse it with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Extract data using the provided selectors\n",
    "        title = soup.select_one(\"#header > div > div.ui-pdp-header__title-container > h1\")\n",
    "        subtitle = soup.select_one(\"#header > div > div.ui-pdp-header__subtitle > span\")\n",
    "        price = soup.select_one(\"#price > div > div > div > span > span > span.andes-money-amount__fraction\")\n",
    "        currency = soup.select_one(\"#price > div > div > div > span > span > span.andes-money-amount__currency-symbol\")\n",
    "        common_expenses = soup.select_one(\"#maintenance_fee_vis > p\")\n",
    "        squared_meters = soup.select_one(\"#highlighted_specs_res > div > div:nth-child(1) > span\")\n",
    "        dorms = soup.select_one(\"#highlighted_specs_res > div > div:nth-child(2) > span\")\n",
    "        bathrooms = soup.select_one(\"#highlighted_specs_res > div > div:nth-child(3) > span\")\n",
    "        location = soup.select_one(\"#location > div > div.ui-pdp-media.ui-vip-location__subtitle.ui-pdp-color--BLACK > div > p\")\n",
    "\n",
    "        # Extract coordinates from the map image srcset\n",
    "        map_img = soup.select_one(\"#ui-vip-location__map > div > img\")\n",
    "        coordinates = None\n",
    "        if map_img and 'srcset' in map_img.attrs:\n",
    "            srcset = map_img['srcset']\n",
    "            if \"center=\" in srcset:\n",
    "                coordinates = srcset.split(\"center=\")[1].split(\"&\")[0]  # Extract lat,lng\n",
    "                coordinates = coordinates.replace(\"%2C\", \",\")\n",
    "\n",
    "        # Extract tables\n",
    "        tables = soup.find_all(\"tbody\", class_=\"andes-table__body\")\n",
    "\n",
    "        flat_table_data = {}\n",
    "        for table in tables:\n",
    "            rows = table.find_all(\"tr\", class_=\"andes-table__row ui-vpp-striped-specs__row\")\n",
    "            for row in rows:\n",
    "                header = row.find(\"th\", class_=\"andes-table__header\").text.strip() if row.find(\"th\", class_=\"andes-table__header\") else None\n",
    "                value = row.find(\"td\", class_=\"andes-table__column\").text.strip() if row.find(\"td\", class_=\"andes-table__column\") else None\n",
    "                if header and value:\n",
    "                    flat_table_data[header] = value\n",
    "\n",
    "        # Extract description\n",
    "        description = soup.select_one(\"#description > div > div > div > p\")\n",
    "\n",
    "        # Extract verified seller info\n",
    "        verified_seller = soup.select_one(\"#header > div > div.ui-pdp-seller-validated > p > a\")\n",
    "\n",
    "        # Extract image URL\n",
    "        image = soup.select_one(\"#gallery > div > div > span:nth-child(3) > figure > img\")\n",
    "        image_url = image['src'] if image else None\n",
    "\n",
    "        # Append the extracted data to the list\n",
    "        data_entry = {\n",
    "            \"url\": url,\n",
    "            \"title\": title.text.strip() if title else None,\n",
    "            \"subtitle\": subtitle.text.strip() if subtitle else None,\n",
    "            \"price\": int(price.text.strip().replace(\".\", \"\")) if price else None,\n",
    "            \"currency\": currency.text.strip() if currency else None,\n",
    "            \"common_expenses\": common_expenses.text.strip() if common_expenses else None,\n",
    "            \"squared_meters\": squared_meters.text.strip() if squared_meters else None,\n",
    "            \"dorms\": dorms.text.strip() if dorms else None,\n",
    "            \"bathrooms\": bathrooms.text.strip() if bathrooms else None,\n",
    "            \"location\": location.text.strip() if location else None,\n",
    "            \"coordinates\": coordinates,\n",
    "            \"description\": description.text.strip() if description else None,\n",
    "            \"verified_seller\": verified_seller.text.strip() if verified_seller else None,\n",
    "            \"image_url\": image_url\n",
    "        }\n",
    "\n",
    "        data_entry.update(flat_table_data)\n",
    "        scraped_data.append(data_entry)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "# Quit the Selenium driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and data saved to /Users/felipemediavillalevinson/Documents/Hawspred/data/raw/scraped_apartments_portal_inmob.csv.\n"
     ]
    }
   ],
   "source": [
    "# Check if `scraped_df` exists and update/append data accordingly\n",
    "try:\n",
    "    # If `scraped_df` already exists, append new data\n",
    "    new_data_df = pd.DataFrame(scraped_data)\n",
    "    scraped_df = pd.concat([scraped_df, new_data_df], ignore_index=True).drop_duplicates(subset=\"url\")\n",
    "except NameError:\n",
    "    # If `scraped_df` doesn't exist, create it from `scraped_data`\n",
    "    scraped_df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Save the scraped data to a CSV file\n",
    "output_file = parent_directory / \"data\" / \"raw\" / 'scraped_apartments_portal_inmob.csv'\n",
    "scraped_df.to_csv(output_file, index=False)\n",
    "print(f\"Scraping completed and data saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quit any remaining chrome instances\n",
    "import os\n",
    "import signal\n",
    "import psutil\n",
    "\n",
    "def kill_chrome_instances():\n",
    "    for process in psutil.process_iter(attrs=['pid', 'name']):\n",
    "        # Look for Chrome or chromedriver processes\n",
    "        if 'chrome' in process.info['name'].lower() or 'chromedriver' in process.info['name'].lower():\n",
    "            try:\n",
    "                os.kill(process.info['pid'], signal.SIGTERM)\n",
    "                print(f\"Terminated process {process.info['name']} (PID: {process.info['pid']})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not terminate process {process.info['name']} (PID: {process.info['pid']}): {e}\")\n",
    "\n",
    "# Call the cleanup function\n",
    "kill_chrome_instances()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
